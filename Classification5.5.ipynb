{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3ffadfa-d962-49d2-8b1c-c0ddb67d4333",
   "metadata": {
    "panel-layout": {
     "height": 51,
     "visible": true,
     "width": 100
    }
   },
   "source": [
    "Intro "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da4531ea-6598-4329-a7f4-5c363b7ccbe6",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b0446d-5041-4db8-93a5-d2fd51ab94ff",
   "metadata": {},
   "source": [
    "Loading Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c449fe5e-3654-4ed6-9420-00395df1f8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pandas\n",
    "import pandas as pd\n",
    "col_names = ['Age', 'Sex', 'Chest Pain Type', 'Resting Blood Pressure', 'serum cholestoral in mg/dl', 'fasting blood sugar > 120 mg/dl', 'Testing electrocardiographic results (values 0,1,2)', 'maximum heart rate achieved', 'exercise induced angina', 'oldpeak = ST depression induced by exercise relative to rest', 'the slope of the peak exercise ST segment', 'number of major vessels (0-3) colored by flourosopy', 'thal: 0 = normal; 1 = fixed defect; 2 = reversable defect', 'target    Have disease or not (1 = yes; 0 = no)']\n",
    "# load dataset\n",
    "pima = pd.read_csv(\"Heart disease Prediction dataset.csv\", header=None, names=col_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971e1346-147f-4184-9e7e-f229cb0d93dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pima.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b97c4e-856f-4d5d-94bf-ce403b38cec9",
   "metadata": {},
   "source": [
    "Selecting Features\n",
    "\n",
    "Here, you need to divide the given columns into two types of variables dependent(or target variable) and independent variable(or feature variables)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be48ba32-e9ed-4127-b177-e3f0af96c339",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split dataset in features and target variable\n",
    "feature_cols = ['Age', 'Sex', 'Chest Pain Type', 'Resting Blood Pressure', 'serum cholestoral in mg/dl', 'fasting blood sugar > 120 mg/dl', 'Testing electrocardiographic results (values 0,1,2)', 'maximum heart rate achieved', 'exercise induced angina', 'oldpeak = ST depression induced by exercise relative to rest', 'the slope of the peak exercise ST segment', 'number of major vessels (0-3) colored by flourosopy', 'thal: 0 = normal; 1 = fixed defect; 2 = reversable defect', 'target    Have disease or not (1 = yes; 0 = no)']\n",
    "X = pima[feature_cols] # Features\n",
    "y = pima.label # Target variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f86bdb-aa3d-4e2a-946c-c54d24fc6909",
   "metadata": {},
   "source": [
    "Splitting data\n",
    "\n",
    "To understand model performance, dividing the dataset into a training set and a test set is a good strategy.\n",
    "\n",
    "Let's split the dataset by using the function train_test_split(). You need to pass 3 parameters: features, target, and test_set size. Additionally, you can use random_state to select records randomly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e43318e-03d5-44ed-bc57-7368fb99ec8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split X and y into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30380eda-a42d-470e-a63e-8f4a40f54972",
   "metadata": {},
   "outputs": [],
   "source": [
    "Model development and prediction\n",
    "\n",
    "First, import the LogisticRegression module and create a logistic regression classifier object using the LogisticRegression() function with random_state for reproducibility.\n",
    "\n",
    "Then, fit your model on the train set using fit() and perform prediction on the test set using predict(). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1df165-c4fe-40f3-b810-ba174fb2928c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the class\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# instantiate the model (using the default parameters)\n",
    "logreg = LogisticRegression(random_state=16)\n",
    "\n",
    "# fit the model with data\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "y_pred = logreg.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9a59e8-4f8f-4207-95af-16279d51ced7",
   "metadata": {},
   "source": [
    "Model Evaluation using Confusion Matrix\n",
    "\n",
    "A confusion matrix is a table that is used to evaluate the performance of a classification model. You can also visualize the performance of an algorithm. The fundamental part of a confusion matrix is the number of correct and incorrect predictions summed up class-wise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7bd133-7ffe-4384-947d-6ab335ed4dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the metrics class\n",
    "from sklearn import metrics\n",
    "\n",
    "cnf_matrix = metrics.confusion_matrix(y_test, y_pred)\n",
    "cnf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f84258-84d1-476a-a8ac-19bfd35a222e",
   "metadata": {},
   "outputs": [],
   "source": [
    "array([[115,   8],\n",
    "      [ 30,  39]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bef3205-b70f-4836-8adb-e9d38c7a9a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Visualizing confusion matrix using a heatmap\n",
    "\n",
    "Let's visualize the results of the model in the form of a confusion matrix using matplotlib and seaborn.\n",
    "\n",
    "Here, you will visualize the confusion matrix using Heatmap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c367391-1049-4de0-a573-d33d163a2485",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required modules\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "class_names=[0,1] # name  of classes\n",
    "fig, ax = plt.subplots()\n",
    "tick_marks = np.arange(len(class_names))\n",
    "plt.xticks(tick_marks, class_names)\n",
    "plt.yticks(tick_marks, class_names)\n",
    "# create heatmap\n",
    "sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap=\"YlGnBu\" ,fmt='g')\n",
    "ax.xaxis.set_label_position(\"top\")\n",
    "plt.tight_layout()\n",
    "plt.title('Confusion matrix', y=1.1)\n",
    "plt.ylabel('Actual label')\n",
    "plt.xlabel('Predicted label')\n",
    "\n",
    "Text(0.5,257.44,'Predicted label');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f8e173-39fc-4443-84c4-175581f652e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "panel-cell-order": [
   "a3ffadfa-d962-49d2-8b1c-c0ddb67d4333"
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
